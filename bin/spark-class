#!/usr/bin/env bash

#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# -z 判断后面参数是否为空 为空则真
# 判断spark_home是否有值，没有的话则调用当前脚本 spark-submit 所在目录下的 find-spark-home 脚本
if [ -z "${SPARK_HOME}" ]; then
  source "$(dirname "$0")"/find-spark-home
fi
#执行spark_home下的 load-spark-env.sh 脚本 设定scala的版本变量值 SPARK_SCALA_VERSION
. "${SPARK_HOME}"/bin/load-spark-env.sh

# Find the java binary
# -n 代表检测变量长度是否为0 不为0时候为真
# 判断java_home 长度是否为0，如果已经安装Java没有设置JAVA_HOME,command -v java返回的值为${JAVA_HOME}/bin/java
if [ -n "${JAVA_HOME}" ]; then
  RUNNER="${JAVA_HOME}/bin/java"
else
  if [ "$(command -v java)" ]; then
    RUNNER="java"
  else
    echo "JAVA_HOME is not set" >&2
    exit 1
  fi
fi

# Find Spark jars.
# -d 检查文件是否为目录，若为目录则为真
#查找SPARK_JARS_DIR，若${SPARK_HOME}/jars 文件目录存在，则SPARK_JARS_DIR="${SPARK_HOME}/jars"，否则
#SPARK_JARS_DIR="${SPARK_HOME}/assembly/target/scala-$SPARK_SCALA_VERSION/jars"
if [ -d "${SPARK_HOME}/jars" ]; then
  SPARK_JARS_DIR="${SPARK_HOME}/jars"
else
  SPARK_JARS_DIR="${SPARK_HOME}/assembly/target/scala-$SPARK_SCALA_VERSION/jars"
fi
#若SPARK_JARS_DIR不存在且$SPARK_TESTING$SPARK_SQL_TESTING有值 [注：一般我们不设置这两变量]，报错退出，否则LAUNCH_CLASSPATH="$SPARK_JARS_DIR/*"
if [ ! -d "$SPARK_JARS_DIR" ] && [ -z "$SPARK_TESTING$SPARK_SQL_TESTING" ]; then
  echo "Failed to find Spark jars directory ($SPARK_JARS_DIR)." 1>&2
  echo "You need to build Spark with the target \"package\" before running this program." 1>&2
  exit 1
else
  LAUNCH_CLASSPATH="$SPARK_JARS_DIR/*"
fi

# Add the launcher build dir to the classpath if requested.
if [ -n "$SPARK_PREPEND_CLASSES" ]; then
  LAUNCH_CLASSPATH="${SPARK_HOME}/launcher/target/scala-$SPARK_SCALA_VERSION/classes:$LAUNCH_CLASSPATH"
fi

# For tests
if [[ -n "$SPARK_TESTING" ]]; then
  unset YARN_CONF_DIR
  unset HADOOP_CONF_DIR
fi

# The launcher library will print arguments separated by a NULL character, to allow arguments with
# characters that would be otherwise interpreted by the shell. Read that in a while loop, populating
# an array that will be used to exec the final command.
#
# The exit code of the launcher is appended to the output, so the parent shell removes it from the
# command array and checks the value to see if the launcher succeeded.
# 执行类文件 org.apache.spark.launcher.Main 返回解析后的参数
# 这里执行的是 launcher module下的 org/apache/spark/launcher/Main.java
# $? 表示最后运行的命令的结束代码(返回值) 即上一个指令的返回值(显示最后命令的退出状态，0表示没有错误，其他任何值表明有错误)

build_command() {
  "$RUNNER" -Xmx128m -cp "$LAUNCH_CLASSPATH" org.apache.spark.launcher.Main "$@"
  printf "%d\0" $?
}

# "$RUNNER"            /opt/module/jdk1.8.0_144/bin/java
# "$LAUNCH_CLASSPATH"  /opt/module/spark/jars/*
# "$@"                 org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode cluster --executor-memory 500m --executor-cores 1 --class org.apache.spark.examples.SparkPi /opt/module/spark/examples/jars/spark-examples_2.11-2.2.0.jar 1000



# Turn off posix mode since it does not allow process substitution
#将build_command方法解析后的参数赋给CMD
# set +o 表示关闭，-o 表示打开，posix 是一种在Unix系统上的软件接口标准，为跨平台兼容使用，支持这种标准的软件可以在所有Unix操作系统上移植使用
# IFS 表示输入域分隔符(input field Separator  ) read -d '' 表示以空格为界定符
# read 表示读取以IFS作为分隔符的行(可能是反斜杠分割的行)
# -r 表示删除反斜杠处理这一过程，保留反斜杠
# CMD 是一个数组，将build_command函数的输出循环读入数组
set +o posix
CMD=()
while IFS= read -d '' -r ARG; do
  CMD+=("$ARG")
#调用 "$RUNNER" -Xmx128m -cp "$LAUNCH_CLASSPATH" org.apache.spark.launcher.Main "$@"
#入口类即 org.apache.spark.launcher.Main
done < <(build_command "$@")
# "$ARG"    /opt/module/jdk1.8.0_144/bin/java
#            -cp
#            /opt/module/spark/conf/:/opt/module/spark/jars/*:/opt/module/hadoop-2.7.7/etc/hadoop/
#            org.apache.spark.deploy.SparkSubmit
#            --master
#            yarn
#            --deploy-mode
#            cluster
#            --class
#            org.apache.spark.examples.SparkPi
#            --executor-memory
#            500m
#            --executor-cores
#            1
#            /opt/module/spark/examples/jars/spark-examples_2.11-2.2.0.jar
#            1000
#            0

# 命令长度
COUNT=${#CMD[@]}
#数组最后一个元素下标
LAST=$((COUNT - 1))
# 数组最后一个值表示build_command执行返回值
LAUNCHER_EXIT_CODE=${CMD[$LAST]}

# Certain JVM failures result in errors being printed to stdout (instead of stderr), which causes
# the code that parses the output of the launcher to get confused. In those cases, check if the
# exit code is an integer, and if it's not, handle it as a special error case.
if ! [[ $LAUNCHER_EXIT_CODE =~ ^[0-9]+$ ]]; then
  echo "${CMD[@]}" | head -n-1 1>&2
  exit 1
fi

if [ $LAUNCHER_EXIT_CODE != 0 ]; then
  exit $LAUNCHER_EXIT_CODE
fi
# CMD： /opt/module/jdk1.8.0_144/bin/java
CMD=("${CMD[@]:0:$LAST}")
# ${CMD[@]}: /opt/module/jdk1.8.0_144/bin/java -cp /opt/module/spark/conf/:/opt/module/spark/jars/*:/opt/module/hadoop-2.7.7/etc/hadoop/ org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode cluster --class org.apache.spark.examples.SparkPi --executor-memory 500m --executor-cores 1 /opt/module/spark/examples/jars/spark-examples_2.11-2.2.0.jar 1000
exec "${CMD[@]}"
# 至此，spark-submit 和spark-class 脚本的前期准备工作已经做完。
# 既然已经知道执行的是 org.apache.spark.deploy.SparkSubmit 这个类，那就跳到这个类继续跟踪